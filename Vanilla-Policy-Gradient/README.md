# Vanilla-Policy-Gradient

## About:
reimplementation of vanilla policy gradient applied to the CartPole-v0 environment from the OpenAI gym<br/>

## Requirements:

Pytorch: <br/>

OpenAI Gym: <br/>

## Model:

Explain what model was used and include a diagram

## Environment:

Explain the data used and short analysis with graphs.

## Training:

Explain training methods, and plot accuracy and loss through training.

## Results:

Show end result accuracy with prediction plot.

## Sources:

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[I'm a relative reference to a repository file](../blob/master/LICENSE)

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <http://www.example.com> and sometimes
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

### Articles
* OpenAI SpinningUp (https://spinningup.openai.com/en/latest/algorithms/vpg.html)

### Papers
* Policy Gradient Methods for Reinforcement Learning with Function Approximation <br/>
  (https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)
