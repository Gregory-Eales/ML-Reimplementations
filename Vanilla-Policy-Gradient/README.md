# Vanilla-Policy-Gradient

## About
reimplementation of vanilla policy gradient applied to the CartPole-v0 environment from the OpenAI gym<br/>

## Requirements

Pytorch: <br/>

OpenAI Gym: <br/>

## Model

Explain what model was used and include a diagram

## Environment

<p align="center">
  <img width="460" height="300" src="https://github.com/Gregory-Eales/ML-Reimplementations/blob/master/Vanilla-Policy-Gradient/img/CartPole-v1.gif">
</p>

Explain the data used and short analysis with graphs.

## Training

Explain training methods, and plot accuracy and loss through training.

## Results

Show end result accuracy with prediction plot.

## Sources

### Articles
* OpenAI SpinningUp (https://spinningup.openai.com/en/latest/algorithms/vpg.html)

### Papers
* Policy Gradient Methods for Reinforcement Learning with Function Approximation <br/>
  (https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)

## Meta

Gregory Eales – [@GregoryHamE](https://twitter.com/GregoryHamE) – gregory.hamilton.e@gmail.com

Distributed under the MIT license. See ``LICENSE`` for more information.

[https://github.com/Gregory-Eales](https://github.com/Gregory-Eales)
